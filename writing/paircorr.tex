\documentclass[11pt,letterpaper]{article}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{natbib}      % http://merkel.zoneo.net/Latex/natbib.php
\usepackage{palatino}
\bibpunct{(}{)}{;}{a}{,}{,}
\usepackage{chngpage}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{subfigure}
\usepackage[usenames,dvipsnames]{color}
\definecolor{myblue}{rgb}{0,0.1,0.6}
\definecolor{mygreen}{rgb}{0,0.3,0.1}
\usepackage[colorlinks=true,linkcolor=black,citecolor=mygreen,urlcolor=myblue]{hyperref}
\newcommand{\bocomment}[1]{\textcolor{Bittersweet}{[#1 -BTO]}}
\newenvironment{itemizesquish}{\begin{list}{\labelitemi}{\setlength{\itemsep}{0em}\setlength{\labelwidth}{2em}\setlength{\leftmargin}{\labelwidth}\addtolength{\leftmargin}{\labelsep}}}{\end{list}}
\newcommand{\ignore}[1]{}
\newcommand{\transpose}{^\mathsf{T}}
\newcommand{\inner}[1]{\langle #1 \rangle}
\newcommand{\smallsec}[1]{\noindent \textbf{#1\ }}

\newcommand{\solution}[1]{{\color{Blue}[\textbf{Solution:} #1]}}
% \newcommand{\solution}[1]{}
\theoremstyle{definition}
\newtheorem{question}{Question}[section]
% \newtheorem{question}{Question}

\title{
Formal properties of word pair correlations
}

\author{
}

%\date{December 9, 2009}

\begin{document}
\maketitle

\section{Introduction}

The idea for this project came when I first heard about a (supposedly) famous mathematical conjecture known as the Gaussian Unitary Ensemble Hypothesis. The essential claim of the GUE hypothesis is that the relative spacings of eigenvalues of a very large random matrix of a certain type (that is, a Gaussian Unitary Ensemble) should resemble, or be underlying identical, to the relative spacings of zeros of the Riemann zeta function, which is used to predict prime numbers. 

Since the 50s, this same particular spacing pattern has also been known to describe the scattering resonances of very heavy nuclei (though what exactly a scattering resonance is is not relevant to this project), and, recently, to describe several other phenomena that can be represented as events on a one-dimensional spectrum, such as birds resting on power lines, bus arrival times, and parallel parked cars. 

This led me to wonder about the spacing of words, to see if they too could be described in this way. As it turns out, the technique used in the GUE hypothesis that allows for the quantitative description of one-dimensional spacing patterns is known as the pair correlation function, commonly written as g(r).

The pair correlation function describes how the density of certain events, or, let's say, particles in solution, varies as a function of distance from a reference particle. To do this, $g(r)$ gives the probability of finding a particle at a distance $r$ from another particle. The formal, general definition of $g(r)$ restricts $r$ to continuous variables in any number of dimensions and involves some calculus. As a result, due to the discrete, combinatorial nature of language, a slightly altered interpretation of the pair correlation function is necessary.

\section{Word Pair Correlation Function}

The ``word pair correlation function'' can be defined for a single word type, or across all words.

Across all words, let's define it as the probability a pair of tokens, spaced $r$ distance apart, are the same word.  So if you picked a position $t$ at random, what's the chance the same word is at $t+r$?  (Restriction: you sample $t$ only from positions $\{1..N-r\}$ where $N$ is the number of tokens in the corpus.)  This is:

\[ g(r) = p(w_{t+r} = w_{t}) \]

For a specific word $v$, there are two possible ways to define it.  We should standardize on one.
\[ g_{cond}(r,v) = p(w_{t+r} = v \mid  w_t = v) \]
versus
\[ g_{joint}(r,v) = p(w_{t+r} = v,\ w_t = v) \]
the joint version (the latter) has much smaller numerical values than the first, and they're especially smaller for rare words. For this reason, we'll use the conditional version, as its values will be easier to interpret.

Finally, $g(r)$ could be defined for a set of words --- for example, to analyze a single paircorr function curve for all names, or all verbs, etc. In fact, the word set paircorr has two versions, a $set-inclusive$ version and a $set-exclusive$ version.

Let the set of words be $A$. 
For the set-inclusive version, the paircorr would look like this:
\[ g(r,A) = p(w_{t+r} \in A \mid w_t \in A) \]
This is equivalent to rewriting all words in the set $A$ to a special symbol, and computing the word paircorr for that symbol (specifically $g_{cond}$).

For the set-exclusive version, the paircorr would look like this:
\[ g(r) = p(w_{t+r} = w_{t} \mid w_t \in A)  \]
This is equivalent to rewriting all words $not$ in the set $A$ to a special symbol, and ignoring that symbol when evaluating the pair correlation.


\section{Pair Correlations of famous texts}

Just to get an idea, 

\section{Pair correlation under Markov models}

Consider the case $r=1$.  $g(1)$ can be rewritten in a history conditional form as follows.

\begin{align}
g(1) &= p(w_{t+1}=w_{t}) \\
&= \sum_v p(w_{t+1}=v,\ w_{t}=v)  \label{e:introduce-v} \\
&= \sum_v p(w_{t+1}=v \mid w_t=v)\ p(w_t=v) \label{e:condprob}
\end{align}

Step \ref{e:introduce-v} decomposes the probability into the sum over all words that are possible.  Step \ref{e:condprob} is definition of conditional probability.


A zeroth order (independent unigrams) assumption yields the following equivalency:
\[p(w_{t+1}=v \mid w_t=v) = p(w_{t+1}=v) = p(w_{t}=v)\]
With this, we can simplify step \ref{e:condprob} and fully derive $g(r)$ under the zeroth order assumption.

\begin{align}
&\sum_v p(w_{t+1}=v \mid w_t=v)\ p(w_t=v) \\
= &\sum_v p(w_{t+1}=v)\ p(w_t=v)\\
= &\sum_v [p(w_{t}=v)]^2  \\
= &g(r) 
\end{align}

Let's consider a first-order (bigram) model. For $r=1$, $g(r)$ is already defined for each word type, by definition of a first-order model. So:
\begin{align}
p(w_{t+1} = w_{t}) &=  \sum_v p(w_{t+1} = v \mid w_{t} = v)\\
p(w_{t+2} = w_{t}) &=  \sum_v p(w_{t+2} = v , w_{t} = v) * p(w_{t} = v) \label{e:sumv}
\end{align}

 Now, the first probability under summation in the previous line, $p(w_{t+2} = v , w_{t} = v)$ can be derived as follows:

\begin{align}
  &p(w_{t+2} = v , w_{t} = v)\\
= &p(w_{t+2} = v  \mid w_{t} = v)\\
= \sum_u &p(w_{t+2} = v , w_{t+1} = u \mid w_{t} = v)\\
= \sum_u &p(w_{t+2} = v \mid w_{t+1} = u , w_{t} = v) * p(w_{t+1} = u \mid w_{t} = v)\\
= \sum_u &p(w_{t+2} = v \mid w_{t+1} = u ) * p(w_{t+1} = u \mid w_{t} = v) \label{e:sumu}
\end{align}

When we combine \ref{e:sum} with \ref{e:sumv}, we get $g(2)$ under a first order model:


\[ \sum_v [\sum_u p(w_{t+2} = v \mid w_{t+1} = u ) * p(w_{t+1} = u \mid w_{t} = v)] * p(w_{t} = v) \]

Without applying any new principles, this expression can be extended to $r = 3$:


\[ \sum_v [\sum_u [\sum_q [p(w_{t+3} = v \mid w_{t+2} = q ) * p(w_{t+2} = q \mid w_{t+1} = u)] p(w_{t+1} = u \mid w_{t} = v) ] p(w_{t} = v) \]

and all other $r$ (with a slight change in notation):

\begin{align} g(r) &= \sum_{v_0}[ \sum_{v_1} [ ... [ \sum_{v_{r-1}} [ p(w_{t+r} = v_0 \mid w_{t+r-1} = v_{r-1}) p(w_{t+r-1} = v_{r-1} \mid w_{t+r-2} = v_{r-2})] \\
&* p(w_{t+r-2} = v_{r-2} \mid w_{t+r-3} = v_{r-3})] p(w_{t+r-3} = v_{r-3} \mid w_{t+r-4} = v_{r-4})] ... ] p(w_t = v_0) 
\end{align}








To do:
\begin{enumerate}
\item Derive $g(r)$ under an arbitrary order model .. or maybe just second order, that sounds easier.
\end{enumerate}


% \bibliographystyle{plainnat}
% \bibliography{brenocon}

\end{document}
